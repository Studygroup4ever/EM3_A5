---
title: "Assignment 5 - Meta-analysis of pitch in schizophrenia"
author: "Riccardo Fusaroli"
date: "3/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(readxl, tidyverse, tidyr, lmerTest, lme4, metafor, dmetar)
```

# Building on the shoulders of giants: meta-analysis
## Questions to be answered

1. What is the current evidence for distinctive vocal patterns in schizophrenia? Report how many papers report quantitative estimates, comment on what percentage of the overall studies reviewed they represent (see PRISMA chart) your method to analyze them, the estimated effect size of the difference (mean effect size and standard error) and forest plots representing it. 

N.B. Only measures of pitch mean and pitch sd are required for the assignment (all the other measures have been removed from the data-set for the sake of simplicity). 

2. Do the results match your own analysis from Assignment 3? If you add your results to the meta-analysis, do the estimated effect sizes change? Report the new estimates and the new forest plots.

3. Assess the quality of the literature: report and comment on heterogeneity of the studies (tau, I2), on publication bias (funnel plot), and on influential studies.

## Tips on the process to follow:

- Download the data on all published articles analyzing voice in schizophrenia and the prisma chart as reference of all articles found and reviewed
Data: https://www.dropbox.com/s/0l9ur0gaabr80a8/Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx?dl=0
Prisma chart: https://www.dropbox.com/s/vbjf6ff73dv9tru/PRISMA%202009%20flow%20diagram_PP.pptx?dl=0 
- Look through the dataset to find out which columns to use, and if there is any additional information written as comments (real world data is always messy!).
    * Hint: PITCH_F0M and PITCH_F0SD group of variables are what you need
- Following the procedure in the slides calculate effect size and standard error of the effect size per each study. N.B. we focus on pitch mean (PITCH_F0_HC_M),(PITCH_F0_SZ_M) and pitch standard deviation (PITCH_F0SD_HC_M),(PITCH_F0SD_SZ_M).
 . first try using lmer (to connect to what you know of mixed effects models)
 . then use rma() (to get some juicy additional statistics)
 
- Build a forest plot of the results (forest(model))
 
- Go back to Assignment 3, add your own study to the data table, and re-run meta-analysis. Do the results change?

- Now look at the output of rma() and check tau and I2

```{r}
# Setup

# Read in excel file
df <- read_xlsx("Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx")

# Counting how long the columns are without NA's
df %>% summarise(
  PitchF0HC = length(na.omit(PITCH_F0_HC_M)),
  PitchF0SZ = length(na.omit(PITCH_F0_SZ_M)),
  PitchF0SDHC = length(na.omit(PITCH_F0SD_HC_M)),
  PitchF0SDSZ = length(na.omit(PITCH_F0SD_SZ_M))
)
```
So, we have 6 complete case-studies for Pitch-mean in healthy controls, 10 complete ones in Pitch-mean in schizophrenics, 15 complete ones in Pitch-standard-deviation-mean in healthy controls (there will be overlap with the 6) and then 20 complete ones in Pitch-standard-deviation mean in schizophrenics (again, there will be overlap here with the 10).


# Creating effect size (yi) and sampling variance (vi) columns
This function "escalc" creates two new columns, where yi is the observed effect size and vi is the variance which we can use in our models. SMD = standardized  mean difference, which is Cohen's d - a measure of effect size. We want it standardized because that allows us to compare across studies, and this is what we do in meta analyses. 

```{r}
# Creating effect size and sampling variance columns using escalc
WM_d <-
  escalc(
    measure = "SMD",
    n1i = SAMPLE_SIZE_HC,
    n2i = SAMPLE_SIZE_SZ,
    m1i = PITCH_F0_HC_M,
    m2i = PITCH_F0_SZ_M,
    sd1i = PITCH_F0SD_HC_M,
    sd2i = PITCH_F0SD_SZ_M,
    data = df
  )

# We can only get effect size estimates from 2 studies - study 1 and 50, because they are the only rows where there are both the means and the sd values: (complete)
test <-
  WM_d %>% select(StudyID,
                  yi,
                  vi,
                  PITCH_F0_HC_M,
                  PITCH_F0_SZ_M,
                  PITCH_F0SD_HC_M,
                  PITCH_F0SD_SZ_M)

# Trying other SD columns now, it works! 6 effect sizes now :) wooow :) SO MANY. Delete the two codelines above when youre ready to not see the mistakes of yore
WM_d <-
  escalc(
    measure = "SMD",
    n1i = SAMPLE_SIZE_HC,
    n2i = SAMPLE_SIZE_SZ,
    m1i = PITCH_F0_HC_M,
    m2i = PITCH_F0_SZ_M,
    sd1i = PITCH_F0_HC_SD,
    sd2i = PITCH_F0_SZ_SD,
    data = df
  )

# Wee see now
test <-
  WM_d %>% select(StudyID,
                  yi,
                  vi,
                  PITCH_F0_HC_M,
                  PITCH_F0_SZ_M,
                  PITCH_F0SD_HC_M,
                  PITCH_F0SD_SZ_M)

```

# MODEL THAT
MODELS FROM SLIDES
```{r}
# Mixed effects implementation
m <-
  lmer( yi ~ 1 + (1 | StudyID),
        weights = 1 / vi,
        data = WM_d,
        control = lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore")
        ) #Model may not have converged with 1 eigenvalue close to zero: 7.2e-10

summary(m)

#Meta-analysis optimization:
# The rma tells R that you want to do a random effects meta-analysis 
m2 <- rma(yi, vi, data = WM_d, slab = Article) # Slab = random effect
?rma
funnel(x = m2, back = 'DarkGreen', shade = 'lightGreen')

forest(m2)
summary(m2)
ranktest(m2)


# What if we have a fixed effect?
m3 <-
  lmer(yi ~ 1 + TYPE_OF_TASK + (1 | StudyID),
       weights = 1 / vi,
       data = WM_d,
       control = lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore")
       )

summary(m3)

inf<- influence(m2)
inf$ids <- rownames(print(inf))
plot(inf)

#m4 <- rma(yi, vi, mods = cbind(Language), data = data, slab=study)

```



```{r}
new <- data.frame(matrix(NA, nrow = 4, ncol = 147))

olddf <- read_csv('pitch_data.csv') %>% filter(Study == 1:4) %>% mutate(Diagnosis = as.factor(Diagnosis),
                                               Study = as.factor(Study),
                                               Subject = as.factor(Subject))



obj <- olddf %>% group_by(Study, Diagnosis) %>% 
  summarise(meanF0 = mean(Mean),
            SDF0 = sd(Mean),
            SampleSize = n_distinct(Subject)
            )

colnames(new) = colnames(df)

new$ArticleID = c(49, 50, 51, 52)
new$StudyID = c(51, 52, 53, 54)

new$Authors = c('Unknown1', 'Unknown2', 'Unknown3', 'Unknown4')

new$PITCH_F0_HC_M = c(obj[1,3], obj[3,3], obj[5,3], obj[7,3])
new$PITCH_F0_SZ_M = c(obj[2,3], obj[4,3], obj[6,3], obj[8,3])

new$PITCH_F0_HC_SD = c(obj[1,4], obj[3,4], obj[5,4], obj[7,4])
new$PITCH_F0_SZ_SD = c(obj[2,4], obj[4,4], obj[6,4], obj[8,4])

new$SAMPLE_SIZE_HC = c(obj[1,5], obj[3,5], obj[5,5], obj[7,5])
new$SAMPLE_SIZE_SZ = c(obj[2,5], obj[4,5], obj[6,5], obj[8,5])

new$Article = c('A3P2 - Study 1', 'A3P2 - Study 2', 'A3P2 - Study 3', 'A3P2 - Study 4')

new_df <- rbind(df, new)
```



```{r}
# Trying other SD columns now, it works! 6 effect sizes now :) wooow :) SO MANY. Delete the two codelines above when youre ready to not see the mistakes of yore
WM_d2 <-
  escalc(
    measure = "SMD",
    n1i = as.numeric(SAMPLE_SIZE_HC),
    n2i = as.numeric(SAMPLE_SIZE_SZ),
    m1i = as.numeric(PITCH_F0_HC_M),
    m2i = as.numeric(PITCH_F0_SZ_M),
    sd1i = as.numeric(PITCH_F0_HC_SD),
    sd2i = as.numeric(PITCH_F0_SZ_SD),
    data = new_df
  )

# Wee see now
test2 <-
  WM_d2 %>% select(StudyID,
                  yi,
                  vi,
                  PITCH_F0_HC_M,
                  PITCH_F0_SZ_M,
                  PITCH_F0_HC_SD,
                  PITCH_F0_SZ_SD)

```


```{r}
# Mixed effects implementation
m11 <-
  lmer( yi ~ 1 + (1 | StudyID),
        weights = 1 / vi,
        data = WM_d2,
        control = lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore")
        ) #Model may not have converged with 1 eigenvalue close to zero: 7.2e-10

summary(m11)

#Meta-analysis optimization:
# The rma tells R that you want to do a random effects meta-analysis 
m21 <- rma(yi, vi, data = WM_d2, slab = Article) # Slab = random effect
?funnel
funnel(x = m21, back = 'DarkRed', shade = 'pink', col = 'Black')

forest(m21)
summary(m21)
ranktest(m21)


# What if we have a fixed effect?
m31 <-
  lmer(yi ~ 1 + TYPE_OF_TASK + (1 | StudyID),
       weights = 1 / vi,
       data = WM_d2,
       control = lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore")
       )

summary(m31)


inf2<- influence(m22)
inf2$ids <- rownames(print(inf2))
plot(inf2)



```









